{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "VYM9ECCdfoOz"
      },
      "outputs": [],
      "source": [
        "# Text generation using GPT-2 model\n",
        "# Load Libraries\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load text generation pipeline with GPT-2\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Prompt for the LLM\n",
        "prompt = \"BERT model is used for Classification\"\n",
        "\n",
        "# Generate text\n",
        "output = generator(prompt, max_length=60, num_return_sequences=1)\n",
        "\n",
        "print(\"Generated Text:\\n\", output[0][\"generated_text\"])\n",
        "\n",
        "# Fine tuning\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SimpleTextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "texts = [\n",
        "    \"I like this movie!\",\n",
        "    \"This film was bad\"]\n",
        "labels = [1, 0]  # 1 = positive, 0 = negative\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "dataset = SimpleTextDataset(texts, labels, tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert-model\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=1,\n",
        "    overwrite_output_dir=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model,args=training_args,train_dataset=dataset)\n",
        "\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
